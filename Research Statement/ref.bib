@inproceedings{10.1145/3400302.3415758,
author = {Sun, Qi and Rao, Arjun Ashok and Yao, Xufeng and Yu, Bei and Hu, Shiyan},
title = {Counteracting Adversarial Attacks in Autonomous Driving},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415758},
doi = {10.1145/3400302.3415758},
abstract = {In this paper, we focus on studying robust deep stereo vision of autonomous driving systems and counteracting adversarial attacks against it. Autonomous system operation requires real-time processing of measurement data which often contain significant uncertainties and noise. Adversarial attacks have been widely studied to simulate these perturbations in recent years. To counteract these attacks in autonomous systems, a novel defense method is proposed in this paper. A stereo-regularizer is proposed to guide the model to learn the implicit relationship between the left and right images of the stereo-vision system. Univariate and multivariate functions are adopted to characterize the relationships between the two input images and the object detection model. The regularizer is then relaxed to its upper bound to improve adversarial robustness. Furthermore, the upper bound is approximated by the remainder of its Taylor expansion to improve the local smoothness of the loss surface. The model parameters are trained via adversarial training with the novel regularization term. Our method exploits basic knowledge from the physical world, i.e., the mutual constraints of the two images in the stereo-based system. As such, outliers can be detected and defended with high accuracy and efficiency. Numerical experiments demonstrate that the proposed method offers superior performance when compared with traditional adversarial training methods in state-of-the-art stereo-based 3D object detection models for autonomous vehicles.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {83},
numpages = {7},
keywords = {autonomous system, robust stereo vision, adversarial defense, local smoothness},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@inproceedings{
tsipras2018robustness,
title={Robustness May Be at Odds with Accuracy},
author={Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SyxAb30cY7},
}
@inproceedings{10.5555/3305381.3305487,
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
title = {Sharp Minima Can Generalize for Deep Nets},
year = {2017},
publisher = {JMLR.org},
abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1019–1028},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@misc{croce2020robustbench,
      title={RobustBench: a standardized adversarial robustness benchmark}, 
      author={Francesco Croce and Maksym Andriushchenko and Vikash Sehwag and Nicolas Flammarion and Mung Chiang and Prateek Mittal and Matthias Hein},
      year={2020},
      eprint={2010.09670},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Andriushchenko:278914,
      title = {Understanding and Improving Fast Adversarial Training},
      author = {Andriushchenko, Maksym and Flammarion, Nicolas},
      journal = {Proceedings of the Advances In Neural Information  Processing Systems 33 (NeurIPS 2020)},
      address = {Advances In Neural Information Processing Systems 33  (NeurIPS 2020)},
      pages = {30},
      year = {2020},
      abstract = {A recent line of work focused on making adversarial  training computationally efficient for deep learning  models. In particular, Wong et al. (2020) showed that  ℓ∞-adversarial training with fast gradient sign method  (FGSM) can fail due to a phenomenon called "catastrophic  overfitting", when the model quickly loses its robustness  over a single epoch of training. We show that adding a  random step to FGSM, as proposed in Wong et al. (2020),  does not prevent catastrophic overfitting, and that  randomness is not important per se -- its main role being  simply to reduce the magnitude of the perturbation.  Moreover, we show that catastrophic overfitting is not  inherent to deep and overparametrized networks, but can  occur in a single-layer convolutional network with a few  filters. In an extreme case, even a single filter can make  the network highly non-linear locally, which is the main  reason why FGSM training fails. Based on this observation,  we propose a new regularization method, GradAlign, that  prevents catastrophic overfitting by explicitly maximizing  the gradient alignment inside the perturbation set and  improves the quality of the FGSM solution. As a result,  GradAlign allows to successfully apply FGSM training also  for larger ℓ∞-perturbations and reduce the gap to  multi-step adversarial training. The code of our  experiments is available at this https URL.},
      url = {http://infoscience.epfl.ch/record/278914},
}

@article{Croce:278915,
      title = {Sparse-RS: a versatile framework for query-efficient  sparse black-box adversarial attacks},
      author = {Croce, Francesco and Andriushchenko, Maksym and Singh,  Naman and Flammarion, Nicolas and Hein, Matthias},
      pages = {20},
      year = {2020},
      abstract = {A large body of research has focused on adversarial  attacks which require to modify all input features with  small l2- or l∞-norms. In this paper we instead focus on  query-efficient sparse attacks in the black-box setting.  Our versatile framework, Sparse-RS, based on random search  achieves state-of-the-art success rate and query efficiency  for different sparse attack models such as l0-bounded  perturbations (outperforming established white-box  methods), adversarial patches, and adversarial framing. We  show the effectiveness of Sparse-RS on different datasets  considering problems from image recognition and malware  detection and multiple variations of sparse threat models,  including targeted and universal perturbations. In  particular Sparse-RS can be used for realistic attacks such  as universal adversarial patch attacks without requiring a  substitute model. The code of our framework is available at  https://github.com/fra31/sparse-rs.},
      url = {http://infoscience.epfl.ch/record/278915},
}

@article{Andriushchenko:273123,
      title = {Square Attack: a query-efficient black-box adversarial  attack via random search},
      author = {Andriushchenko, Maksym and Croce, Francesco and  Flammarion, Nicolas and Hein, Matthias},
      pages = {34},
      year = {2020},
      abstract = {We propose the Square Attack, a new score-based black-box  $l_2$ and $l_\infty$ adversarial attack that does not rely  on local gradient information and thus is not affected by  gradient masking. The Square Attack is based on a  randomized search scheme where we select localized  square-shaped updates at random positions so that the  $l_\infty$- or $l_2$-norm of the perturbation is  approximately equal to the maximal budget at each step. Our  method is algorithmically transparent, robust to the choice  of hyperparameters, and is significantly more query  efficient compared to the more complex state-of-the-art  methods. In particular, on ImageNet we improve the average  query efficiency for various deep networks by a factor of  at least $2$ and up to $7$ compared to the recent  state-of-the-art $l_\infty$-attack of Meunier et al. while  having a higher success rate. The Square Attack can even be  competitive to gradient-based white-box attacks in terms of  success rate. Moreover, we show its utility by breaking a  recently proposed defense based on randomization. The code  of our attack is available at  https://github.com/max-andr/square-attack},
      url = {http://infoscience.epfl.ch/record/273123},
}

@inproceedings{zeng2019adversarial,
  title={Adversarial attacks beyond the image space},
  author={Zeng, Xiaohui and Liu, Chenxi and Wang, Yu-Siang and Qiu, Weichao and Xie, Lingxi and Tai, Yu-Wing and Tang, Chi-Keung and Yuille, Alan L},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4302--4311},
  year={2019}
}
@article{Tsipras2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.12152v5},
author = {Tsipras, Dimitris and Turner, Alexander},
eprint = {arXiv:1805.12152v5},
file = {:Users/arjunrao/Dropbox/Papers-adversarial/Tsipras, Turner - 2018 - Robustness May Be at Odds with Accuracy.pdf:pdf},
mendeley-groups = {Research Statement},
pages = {1--24},
title = {{Robustness May Be at Odds with Accuracy}},
year = {2018}
}
@article{engstrom2019adversarial,
  title={Adversarial robustness as a prior for learned representations},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  journal={arXiv preprint arXiv:1906.00945},
  year={2019}
}
@inproceedings{43405,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}


@inproceedings{stutz2019disentangling,
  title={Disentangling adversarial robustness and generalization},
  author={Stutz, David and Hein, Matthias and Schiele, Bernt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6976--6987},
  year={2019}
}
@inproceedings{chan2020thinks,
  title={What it Thinks is Important is Important: Robustness Transfers through Input Gradients},
  author={Chan, Alvin and Tay, Yi and Ong, Yew-Soon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={332--341},
  year={2020}
}
