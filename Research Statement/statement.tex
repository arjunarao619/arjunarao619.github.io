
\documentclass[12pt]{article}

\usepackage[
  a4paper,
  margin=1in,
  headsep=10pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{datetime}

\newdateformat{monthyeardate}{%
  \monthname[\THEMONTH], \THEYEAR}
\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{} 
\lhead{\footnotesize Arjun Rao}% \lhead puts text in the top left corner. \footnotesize sets our font to a smaller size.
\rhead{\footnotesize Research Statement - \monthyeardate\today } 

\usepackage{graphicx}

\newcommand{\soptitle}{Arjun Ashok Rao}

\newcommand{\yourname}{firstname lastname}
\newcommand{\youremail}{arjunrao@link.cuhk.edu.hk}
\newcommand{\yourweb}{https://arjunashokrao.me}
\newcommand{\citeColored}[2]{{\hypersetup{citecolor=#1}\cite{‌​#2}}}
\newcommand{\HRule}[1][\medskipamount]{\par
  \vspace*{\dimexpr-\parskip-\baselineskip+#1}
  \noindent\rule{\linewidth}{0.3mm}\par
  \vspace*{\dimexpr-\parskip-.3\baselineskip+#1}}

\newcommand{\statement}[1]{\par\medskip
  \textcolor{blue}{\textbf{#1:}}\space
}
\usepackage{hyperref}
\hypersetup{
    colorlinks=True,
    linkcolor={blue},
    citecolor=blue,
    filecolor=red,      
    urlcolor=blue,
}

\begin{document}
\thispagestyle{empty}
\Large \textbf{Arjun Rao} \hfill \Large  \textbf{\textcolor{black}{\Large{Research Statement} }} \\ \textcolor{gray}{\textbf{\hspace*{0.25in}\normalsize{The Chinese University of Hong Kong}}}
\HRule

\small Homepage: \href{https://arjunashokrao.me}{arjunashokrao.me} \quad Email: \href{mailto:arjunrao@link.cuhk.edu.hk}{arjunrao@link.cuhk.edu.hk}

\bigskip

\textbf{Abstract:} My Research Interests include Computer Vision and Machine Learning. More particularly, I am interested in understanding adversarial machine learning from a \emph{robustness} perspective, and how adversarial robustness can shed light on theoretical concepts such as \emph{generalization} as well as \emph{representation learning} and \emph{transfer learning}. My long term research goal is understand generalization guarantees in over-parameterized deep nets, and to develop deep learning models which are robust to adversarial and poisoning attacks in the real world.  \\

Adversarial Examples, which can better be visualized as imperceptible `distributional' shifts in test-datasets are a natural consequence of the dimensionality gap between inputs and linear models on which high-dimensional inputs are trained on.
%Adversarial examples which lie on a low-dimensional data manifold are chief contributors to generalization error \cite{43405,stutz2019disentangling}.
They generalize across different architectures, and can be used in a `black-box' fashion to threaten real-world deep learning models. Recent work has demonstrated the almost ubiquitous prevalence of adversarial examples in different applications of deep neural networks - encompassing classification (Image and voice classification, Facial Recognition), regression (Object detection, multi-object tracking), and reinforcement learning. The most common strategy to defend against test-time attacks has been to train models on adversarial data, thus ensuring some `robustness` against standard attacks. Interestingly, these adversarially trained (robust) models exhibit intriguing properties not seen in standard deep nets. My current research aims on understanding and leveraging these properties to develop deep nets that are interpretable, secure, and accurate on unseen data. 
    \\

\textbf{Robustness and Generalization:} Understanding adversarial robustness may help develop a better understanding about broad theoretical questions such as those on local minima, generalization in over-parameterized networks, or the reasoning behind flat vs sharp global optimums. The consensus regarding loss landscape geometries have been that flatter minima basins lead to better generalization. This hypothesis is a potential explanation to the robustness-accuracy tradeoff in robust models \cite{tsipras2018robustness} since robust models qualitatively exhibit sharper minima with lesser basin volume. 
%This generalization error occurs when the adversarial sample lies within the boundary of the low-dimensional data manifold \cite{zeng2019adversarial}. 
However, the lack of generalization in robust models is not absolute, and a large number of studies which study this generalization gap are only empirical. Explicit regularization like enforcing local linearity of the loss surface combats the sharp minima problem in robust models. It is also possible that the good generalization in robust models can be due special cases of sharp minima generalization \cite{10.5555/3305381.3305487}.\\

\textbf{Robustness and Representations:} My research also studies how robustness leads to interpretable machine learning and why robust networks contain representations that align more with the human assumptions of intermediate neural network features. For example, previous empirical studies have shown that models trained against adversarial examples exhibit more human-perceptible input-gradient visualizations, as well as a smooth interpolation between image classes. Robust representations provide a high-level embedding of the input such that similar-looking classes have intermediate representations that are semantically similar \cite{engstrom2019adversarial}. In fact, performing adversarial training on very small perturbation sets might not even lead to robustness and security, but can still lead to better representations. While the intuition that neural networks truly learn interpretable representations is often assumed to be true, it is not certain. Answering questions on why robustness leads to better representations would equal taking a large step towards demystifying these `black-boxes'. \\

%\textbf{Robustness and Transferability:} The inherent gap in dimensionality between inputs and linear models on which those high-dimensional inputs are trained on is a major explanation for the existence of adversarial examples. In fact, studies claim that adversarial examples which lie on a low-dimensional data manifold are chief contributors to generalization error \cite{43405,stutz2019disentangling}. Since adversarial examples generalize across different architectures, they are transferable and can be used in a `black-box' fashion to threaten real-world deep learning models. However, robustness might also transfer through input gradients \cite{chan2020thinks}

\textbf{Current Research:} At The Chinese University of Hong Kong, my research primarily involves developing application-specific adversarial defenses. Stereo-Vision, most commonly used in autonomous driving applications were shown to be vulnerable to adversarial attacks that primarily distort the \emph{disparity} perception in the rectified stereo pair. My research developed a completely cyber-physical approach to conduct adversarial training with left-right feature map regularization while ensuring local linearity of the loss surface \cite{me}. I also worked on more exploratory research on understanding distributed deep learning and how over-parameterized neural networks can generalize in a distributed setting. My current research is on Optical Character Recognition models and their potential vulnerability to black-box adversarial examples. Many of these approaches are the current state-of-the-art, and consistently have shown to be more robust than previous defenses.  \\

\textbf{Future Research Goals:} Machine Learning has demonstrated incredible progress over the past decade. However, the disproportionate progress in empirical studies opposed to theoretical explanations have to be bridged in the near future. Broadly, I believe that a large part of my research is to understand and evaluate training algorithms for deep neural networks. An integral part of my future research goal is to see through the opaqueness of over-parameterized deep nets. More specifically, I am interested in studying the generalization guarantee that deep neural nets offer on modern datasets. It is understood that SGD - a variation of gradient descent that updates network weights based on a `mini-batch' of input-data performs a certain `implicit' regularization that is responsible for their excellent generalization results. However, we know very little about optimization trajectories that these powerful algorithms take, especially for over-parameterized nets with high-dimensional, highly non-convex loss landscapes. My near-term research goal is to demonstrate and visualize these phenomenon to provide better intuitions for other researchers. Secondly, I am interested in safe and robust machine learning that can be used at scale. My previous work on studying adversarial attacks on autonomous systems is an example of an effort in this interesting and needed research direction. 
%My broad research goal is to develop an understanding of deep neural networks that can potentially explain the presence of these small and imperceptible adversarial examples. This understanding should also help other explain other unexplained dichotomies and findings in adversarial machine learning - like how robust training leads to better representations but worse generalization with sharper global minima in their loss landscapes. 
\bibliographystyle{IEEEtran}
\bibliography{ref.bib}
\end{document}